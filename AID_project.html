<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta charset="utf-8">
    
<title>A Benchmark on Aerial Scene Classification</title>
    
    <style>
        h1 { padding : 0; margin : 0 auto; text-align : center;}
        body { padding : 0; font-family : Arial; font-size : 16px; }
        #container { width : 900px; margin : 10px auto;  background-color : #fff; padding : 50px; box-shadow: 0px 0px 10px #000; border-radius: 15px;  } 
        #me { border : 0 solid black; margin-bottom : 0;}
        #content { display : block;}
        a { text-decoration : none; }
        a:hover { text-decoration : underline; }
        a:visited { color : blue; }
        a.invisible { color : inherit; text-decoration : inherit; }
        .publogo { margin-right : 10px; float : left; border : 0;}
        .publication { clear : left; padding-bottom : 0px;}
        .publication p { height : 100px; }
        .codelogo { margin-right : 10px; float : left; border : 0;}
        .code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
        .code .download a { display : block; margin : 0 15px; float : left;}
        #simpsons { margin : 5px auto; text-align : left; color : #B7B7B7; }
        #erdos { color : #999; text-align : center; font-size : 12px; }
        .contact { margin-left : 40px; }
        .contact td { width : 300px; vertical-align : top; }
        .schoollogo { text-align : center; color : #999; width : 150px;}
        .schoollogo img { margin-bottom : 10px; }
        body {
        background-color : rgb(100,100,100); /*UCLA bgcolor*/
        }
    </style>	
    
</head>

<body>

<div id="container">

<div id="content">

<h1>	A Benchmark on Aerial Scene Classification</h1>
<p style="text-align:center;margin-bottom:30px;">
   <a href="https://sites.google.com/site/link2xgs/home">Gui-Song Xia</a>*<sup>1</sup>,
   Jingwen Hu<sup>1,2</sup>,  
   Fan Hu<sup>1,2</sup>,
   Baoguang Shi<sup>3</sup>,
   Xiang Bai<sup>3</sup>,
   <a href="http://www.lmars.whu.edu.cn/prof_web/zhangliangpei/rs/index.html">Liangpei Zhang</a><sup>1</sup><br>
   {hujingwen,  guisong.xiag,  zlp62}@whu.edu.cn<br>
    1. 
    LIESMARS, Wuhan University,
    Wuhan 430079, China<br>
    2. EIS, Wuhan University,
    Wuhan, 430079, China<br>
    3. EIC, Huazhong University of Science and Technology ,
    Wuhan 430074, China

</p>

<h2>Abstract</h2>
<p style="text-align:justify;margin-bottom:10px;">Aerial scene classification, which aims to automatically label an aerial image with a specific semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent years, it has become a dynamic task in remote sensing area and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. However, the existing datasets for aerial scene classification like UC-Merced dataset and WHU-RS19 are with relatively small sizes, and the results on them are already saturated. This largely limits the development of scene classification algorithms.
This paper describes the Aerial Image Dataset (AID): a large-scale dataset for aerial scene classification.
The goal of AID is to advance the state-of-the-arts in scene classification of remote sensing images. For creating AID, we collect and annotate more than ten thousands aerial scene images. In addition, a comprehensive review of the existing aerial scene classification techniques as well as recent widely-used deep learning methods is given. Finally, we provide a performance analysis of typical aerial scene classification and deep learning approaches on AID, which can be served as the baseline results on this benchmark.
</p>

<h2 style="text-align:justify;margin-bottom:10px;">Dataset</h2>
<p style="text-align:justify;margin-bottom:10px;"> We construct a new and large scale dataset to evaluate the 
performances of the scene classification algorithms more fairly, add more challenges and aim to promote the 
development of new methods. The images in our dataset are multi-source data from different remote sensors, 
which adds more challenge for scene classification than the single source data like
<a href="http://vision.ucmerced.edu/datasets/landuse.html">UC-Merced dataset</a>. 
Moreover, all the sample images in each class are carefully chosen from different countries and regions around
the world, mainly in China, the United States, England, France, Italy, Japan, Germany, etc., and they are 
extracted at different time and seasons under different imaging conditions and pixel resolutions, which can 
help to increase the intra-class diversities. To decrease the inter-class distance, we further add ten types
of challenging scenes in the dataset, i.e., baseball field, center, church, medium residential, playground, 
resort, school, sparse residential, square, storage tanks. Thus, the new dataset is made up of 30 scene types
with 10000 images in all. Some samples of each class are shown in Fig.1.
</p>

<table><tr>
<table><tr><th><td valign=top align=center width=0>
<td align=center>airport<br><a href="Airport.html"><img width="170" src="AID/Airport/airport_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>bareland<br><a href="BareLand.html"><img width="170" src="AID/BareLand/bareland_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>baseballfield<br><a href="BaseballField.html"><img width="170" src="AID/BaseballField/baseballfield_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>beach<br><a href="Beach.html"><img width="170" src="AID/Beach/beach_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>bridge<br><a href="Bridge.html"><img width="170" src="AID/Bridge/bridge_1.jpg"></a> </td><th><td valign=top align=center width=0>
<tr>
<th><td valign=top align=center width=0>
<td align=center>center<br><a href="Center.html"><img width="170" src="AID/Center/center_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>church<br><a href="Church.html"><img width="170" src="AID/Church/church_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>commercial<br><a href="Commercial.html"><img width="170" src="AID/Commercial/commercial_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>denseresidential<br><a href="DenseResidential.html"><img width="170" src="AID/DenseResidential/denseresidential_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>desert<br><a href="Desert.html"><img width="170" src="AID/Desert/desert_1.jpg"></a> </td><th><td valign=top align=center width=0>
<tr>
<th><td valign=top align=center width=0>
<td align=center>farmland<br><a href="Farmland.html"><img width="170" src="AID/Farmland/farmland_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>forest<br><a href="Forest.html"><img width="170" src="AID/Forest/forest_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>industrial<br><a href="Industrial.html"><img width="170" src="AID/Industrial/industrial_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>meadow<br><a href="Meadow.html"><img width="170" src="AID/Meadow/meadow_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>mediumresidential<br><a href="MediumResidential.html"><img width="170" src="AID/MediumResidential/mediumresidential_1.jpg"></a> </td><th><td valign=top align=center width=0>
<tr>
<th><td valign=top align=center width=0>
<td align=center>mountain<br><a href="Mountain.html"><img width="170" src="AID/Mountain/mountain_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>park<br><a href="Park.html"><img width="170" src="AID/Park/park_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>parking<br><a href="Parking.html"><img width="170" src="AID/Parking/parking_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>playground<br><a href="Playground.html"><img width="170" src="AID/Playground/playground_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>pond<br><a href="Pond.html"><img width="170" src="AID/Pond/pond_1.jpg"></a> </td><th><td valign=top align=center width=0>
<tr>
<th><td valign=top align=center width=0>
<td align=center>port<br><a href="Port.html"><img width="170" src="AID/Port/port_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>railwaystation<br><a href="RailwayStation.html"><img width="170" src="AID/RailwayStation/railwaystation_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>resort<br><a href="Resort.html"><img width="170" src="AID/Resort/resort_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>river<br><a href="River.html"><img width="170" src="AID/River/river_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>school<br><a href="School.html"><img width="170" src="AID/School/school_1.jpg"></a> </td><th><td valign=top align=center width=0>
<tr>
<th><td valign=top align=center width=0>
<td align=center>sparseresidential<br><a href="SparseResidential.html"><img width="170" src="AID/SparseResidential/sparseresidential_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>square<br><a href="Square.html"><img width="170" src="AID/Square/square_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>stadium<br><a href="Stadium.html"><img width="170" src="AID/Stadium/stadium_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>storagetanks<br><a href="StorageTanks.html"><img width="170" src="AID/StorageTanks/storagetanks_1.jpg"></a> </td><th><td valign=top align=center width=0>
<td align=center>viaduct<br><a href="Viaduct.html"><img width="170" src="AID/Viaduct/viaduct_1.jpg"></a> </td><th><td valign=top align=center width=0>
<tr>
</table></tr>
<div> <strong>Figure 1:</strong> Samples of the AID dataset: three examples of each semantic scene class 
are shown. There are 10000 images within 30 classes.</div>
    
<h2>Experimental study</h2>
<h3>A. Benchmark methods</h3>
<p>
In our paper, we evaluate the following three kinds of scene classification methods: <br>
<ol>
    <li>
    Low-level methods: Scale Invariant Feature Transform (SIFT), Local Binary Pattern (LBP), Color Histogram (CH) and Gist. <br>
    </li>
    <li>
    Mid-level mrthods: Bag of Visual Words (BoVW), Spatial Pyramid Matching (SPM), Locality-constrained Linear Coding (LLC), 
Probabilistic Latent Semantic Analysis (pLSA), Latent Dirichlet allocation (LDA), Improved Fisher kernel (IFK) and
Vector of Locally Aggregated Descriptors (VLAD) combined with three local feature descriptors (i.e., SIFT, LBP and CH).<br>
    </li>
    <li>
    High-level methods: CaffeNet, VGG-VD-16 and GoogLeNet.<br>
    </li>
</ol>

<h3>B. Tested datasets</h3>
In our paper, we conduct the experiments on three popular scene classification datasets as well as our AID dataset: <br>  
<ol>
     <li>
         <a href="http://vision.ucmerced.edu/datasets/landuse.html">UC-Merced dataset</a>, 
         contains 21 scene classes and 100 samples of size 256x256 in each class.
     </li>
     <li>
         <a href="http://dsp.whu.edu.cn/cn/staff/yw/HRSscene.html"> WHU-RS19 dataset</a>, 
         has 19 different scene classes and 50 samples of size 600x600 in each class.
     </li>
     <li>
         <a href="https://sites.google.com/site/qinzoucn/documents">RSSCN7 dataset</a>, 
         contains 7 scene classes and 400 samples of size 400x400 in each class.
     </li>
     <li>
         <a href="AIDscene.html"> AID dataset</a>, 
         has 30 different scene classes and about 200 to 400 samples of size 600x600 in each class.
     </li>
</ol>

<h3>C. Evaluation protocols</h3>
To compare the classification quantitatively, we compute the common used measures: overall accuracy (OA), 
which is defined as the number of correctly predicted images divided by the total number of predicted images. 
It is a direct measure to reveal the classification performance on the whole dataset.<br>
<br>
To compute OA, we adopt two different settings for each tested dataset in the supervised classification process.
To compare the performances fairly, we adopt the Support Vector Machine (SVM) with linear kernel as our classifier.
For the RSSCN7 dadaset and our benchmark dataset, we fix the ratio of the number of training set to be 20% and 50%
respectively and the left for testing, while for UC-Merced dataset, the ratios are set to be 50% and 80% 
respectively. For the WHU-RS19 dataset, the ratios are fixed at 40% and 60% respectively. To compute the overall
accuracy, we randomly split the datasets into training sets and testing sets for evaluation, and repeat it ten 
times to reduce the influence of the randomness and obtain reliable results. The OA is computed for each run, 
and the final result is reported as the mean and standard deviation of OA from the individual run.

<h3>D. Classification results</h3>
<p align="center"><a href="low.png"><img src="low.png" width="900" height="135"></a><br> 
<strong>Table.1: </strong>Overall accuracy (OA) of different low-level methods on the UC-Merced dataset,
the WHU-RS19 dataset, the RSSCN7 dataset and our new dataset. </p>

<p align="center"><a href="mid.png"><img src="mid.png" width="900" height="500"></a><br> 
<strong>Table.1: </strong>Overall accuracy (OA) of different mid-level methods on the UC-Merced dataset,
the WHU-RS19 dataset, the RSSCN7 dataset and our new dataset. </p>

<p align="center"><a href="high.png"><img src="high.png" width="900" height="110"></a><br> 
<strong>Table.1: </strong>Overall accuracy (OA) of different high-level methods on the UC-Merced dataset,
the WHU-RS19 dataset, the RSSCN7 dataset and our new dataset. </p>

<h3>E. Discussion</h3>
From the above experimental results, we can summarize some interesting but meaningful observations as follows:
<ol>
     <li>
     By comparing various scene classification methods, we can observe the layered performances as the name 
     implies: low-level methods have relatively worse performances, while high-level methods perform better 
     on all the datasets, which shows the great potential of high-level methods.
     </li>
     <li>
     By comparing different scene classification datasets, we can find that our new dataset is far more 
     challenging than the others for it has relatively higher intra-class variations and smaller inter-class
     dissimilarity. In addition, the large numbers of sample images can help to evaluate various methods more
     precisely.
     </li>
</ol>

</p>
</div><br>
    
<strong>The codes can be downloaded from <a href="code.zip"> here</a>. </strong>

<!-- <p align="left"><a style ="text-decoration: underline;" href="../index.html"><- Back to Homepage</a></p>-->
<hr size="2">

<!-- Start of StatCounter Code -->
<script type="text/javascript">
    var sc_project=4056802; 
    var sc_invisible=1; 
    var sc_partition=50; 
    var sc_click_stat=1; 
    var sc_security="b7292327"; 
</script>

<script type="text/javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><div class="statcounter"><a title="web stats" href="http://www.statcounter.com/" target="_blank"><img class="statcounter" src="http://c.statcounter.com/4056802/0/b7292327/1/" alt="web stats" ></a></div></noscript>
<!-- End of StatCounter Code -->

<div id="my_copyright">&#169Gui-Song Xia 2016</div>
